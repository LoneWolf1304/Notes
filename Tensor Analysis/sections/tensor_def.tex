\section{Why are tensors so sigma!}
Nakahara defines tensor as:
\begin{quote}
    \textit{A tensor $T$ of type $(p,q)$ is a multi-linear map that maps $q$ vectors and $p$ dual vectors to $\mathbb{R}$, that is:
    $$ T: \brac{\bigotimes\limits^p \mathbf{V}^* }\bigotimes \brac{\bigotimes\limits^q \mathbf{V} }\to \mathbb{R} $$}
\end{quote}
Dayummm!! \emoji{expressionless-face} Let us break this down. Consider a scalar which has no vector and no dual vector. Thus, it is a $(0,0)$ type tensor. Now, let us consider a vector $\mathbf{v}$. This is a $(1,0)$ tensor, that is, it maps a dual vector to a scalar. If we have a dual vector $\mathbf{f}$, then it is of type $(0,1)$ and maps a vector to a scalar. This does not clear anything. Let us instead consider few examples:\\[0.3cm]
\textbf{Moment of Intertia Tensor:}\\[0.3cm]
Perhaps the first example of a tensor we had encountered during our classical mechanics course (which we had been told to understand just as a `matrix'). 
\begin{figure}[H]
    \centering
    \input{plot&diagrams/rigid.tex}
    \caption{A rigid body rotating about an axis}
\end{figure}
\noindent
Consider a rigid body made of tiny masses $dm$. Consider one such mass sitauted as a distance $s$ from the fixed axis of rotation. It goes around a circle with speed $v = \omega s$. The angular momentum can be calculated as:
$$\mathbf{L} = \int (\veb{r}\times \veb{v})dm = \int (\veb{r}\times (\veb{\omega}\times \veb{r}))dm = \int (\veb{r}\cdot \veb{r})\veb{\omega} -  \cancelto{0}{(\veb{r}\cdot \veb{\omega})}\veb{r}dm =\veb{\omega} \underbrace{\int s^2 dm}_{\text{I}} $$
The integral is called the moment of inertia. In a more general case, where there is no fixed axis of rotation, we write: 
\begin{align*}
    \mathbf{L} &= \int (\veb{r}\times \veb{v})dm \\
    &= \int (\veb{r}\times (\veb{\omega}\times \veb{r}))dm \\
    &= \int (\veb{r}\cdot \veb{r})\veb{\omega} -  {(\veb{r}\cdot \veb{\omega})}\veb{r}dm
\end{align*}
We now write it in index notation, noting that $\omega^i = \tensor*{\delta}{^i^j}\omega_j$:
\begin{align*}
    L^i &= \int (\veb{r}\cdot \veb{r})\tensor*{\delta}{^i^j}\omega_j - x^i(x^j\omega_j)dm\\
    &= \omega_j \brac{\int (\veb{r}\cdot \veb{r})\tensor*{\delta}{^i^j} - x^ix^j\ dm} 
\end{align*}
The integral in the bracket is defined to be the inertia tensor:
$$I^{ij} = \int (\veb{r}\cdot \veb{r})\tensor*{\delta}{^i^j} - x^ix^j \ dm$$
Note that $i$ and $j$ goes from $1$ to $3$ and thus it has $9$ components but since the expression is symmetric, we only have $6$ independent components. This states that the angular momentum and the angular velocity are not necessarily parallel in some coordinate system where $I$ have non-zero off-diagonal entries.\\[0.3cm]
\textbf{Electromagnetic Tensor:}\\[0.3cm]
The electromagnetic tensor is very useful in combining the electric field and magnetic field and finding their transformations. It is defined as:
$$F_{\mu\nu} = \partial_\mu A_\nu - \partial_\nu A_\mu$$
where $A_\mu$ is the 4-potential. The indices $\mu$ and $\nu$ can take values from $0$ to $3$. The tensor has $16$ components but only $6$ of them are independent. The tensor is antisymmetric, that is, $F_{\mu\nu} = -F_{\nu\mu}$ and thus the diagonal entries are zero. We will discuss this later but Maxwell's equations can be written in a very compact form using the components of the electromagnetic tensor.\\[0.3cm]
\textbf{Electric-Susceptibility Tensor:}\\[0.3cm]
We had studied about polarisation in dielectrics in our classical electrodynamics course where we had often taken (for simplicity):
$$\veb{P} = \epsilon_0\chi\veb{E}$$
Here we had taken the electric field to be parallel to the polarisation vector but in general, these are related by the susceptibility tensor as:
$$P^i = \epsilon_0\chi^{ij}E^{j}$$
\subsection{Gradient}
The components of the gradient (basically partial derivative) are covariant. We denote it with a lower index explicitly to show that it is a covariant vector:
$$\partial_\mu \equiv \pdv{x^\mu}$$
Then accordingly we will have:
$$\partial^\mu \equiv \pdv{x_\mu} \quad \quad \partial^\mu = g^{\mu\nu}\partial_\nu$$
\subsection{Tensor Transformations}
As previously seen for vector transformation, the transformation of a tensor from one system to another is very similar. For simplicity, we show for a second rank tensor:
$$\tensor{T }{^{\prime ij}} = \pdv{x^{\prime i}}{x^k} \pdv{x^{\prime j}}{x^n} \tensor{T}{^{kn}}$$
This is simple: in the left everything is prime and contravariant, so in the right the numerator must have primes and the denominator must have unprimed indices and contraction should be done so as to match the contra and co indices on both sides in the final expansion. Well, this transformation law is such a nice thing that many people say that: 
$$\boxed{\mathrm{Tensors\ are \ defined\ in\ the\ way\ they\ transform}}$$
Let us see some more examples of tensors and their transformation from lower to upper indices (it's easy, just use the metric tensor appropriately):
\begin{align*}
    \tensor{T}{^\mu^\nu} &=g^{\rho\mu}g^{\sigma\nu} \tensor{T}{_\rho_\sigma}\\
    \tensor{T}{^\mu _\nu} &=\tensor{g}{^\mu _\rho}\tensor{g}{^\sigma _\nu} \tensor{T}{^\rho _\sigma}
\end{align*}
Lastly we show an example of a mixed tensor transformation from one system to another \emoji{fire}:
\begin{align*}
    \tensor{T}{^{\prime\mu} _\nu _\rho} =\pdv{x^\beta}{x'^\rho}\ \pdv{x^\alpha}{x'^\nu} \ \pdv{x'^\mu}{x^\sigma}  \tensor{T}{^\sigma _\alpha _\beta}
\end{align*}
So basically, while changing the covariant indices, the prime is in the denominator and for contravariant, it is above. And then the positioning of the indices is trivial (I guess...)
\subsection{Matrices vs. Tensor? same same but different....}
Not all matrices are second-rank tensors \emoji{expressionless}. Yes, the components of a second-rank tensor can be arranged in a matrix form but there are many matrices which do not transform according to the above equation. Take for example the following matrix, which we assume as a tensor:
$$[T^{lm}] \equiv \begin{pmatrix}
    (x^2)^2 & x^1x^2\\
    x^1x^2 & (x^2)^2
\end{pmatrix}$$
Note that the 2 outside the bracket is the power and inside the bracket is the index of the component. Then after rotation, we have the following relation:
\begin{align*}
    x'^1 &= x^1\cos\theta + x^2\sin\theta \\
    x'^2 &= -x^1\sin\theta + x^2\cos\theta
\end{align*}
Let us find $T^{'11}$ which according to the transformation rule, should be:
\begin{align*}
    T^{'11} &= \pdv{x'^1}{x^k} \pdv{x'^1}{x^n} T^{kn} \\
    &=\pdv{x'^1}{x^1} \pdv{x'^1}{x^1} T^{11} + \pdv{x'^1}{x^1} \pdv{x'^1}{x^2} T^{12}+\pdv{x'^1}{x^2} \pdv{x'^1}{x^1} T^{21} +\pdv{x'^1}{x^2} \pdv{x'^1}{x^2}T^{22}\\
    &= \cos\theta\cos\theta T^{11} + \cos\theta \sin\theta T^{12} + \sin\theta\cos\theta T^{21} + \sin\theta\sin\theta T^{22} \\
    &= \cos^2\theta (x^2)^2 + 2\sin\theta\cos\theta (x^1)(x^2) + \sin^2\theta (x^2)^2\\
    &=(x^2\cos\theta + x^1\sin\theta)^2
\end{align*}
Well note that if $T$ was indeed a tensor, then $T^{'11}$ should be equal to $(x'^2)^2$ but it is not. Thus, $T$ is not a tensor and our assumption was wrong.  \textbf{Thus all matrices are not tensors!}